{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import cv2\n",
    "#retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs, rew, done, info = env.step(env.action_space.sample())\n",
    "#(224, 240, 3)\n",
    "#MultiBinary(9)\n",
    "#[\"B\", null, \"SELECT\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\"]\n",
    "action_space = [[0,0,0,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,0,0,1],\n",
    "                [1,0,0,0,0,0,1,0,0],\n",
    "                [1,0,0,0,0,0,0,1,0],\n",
    "                [1,0,0,0,0,0,0,0,1],\n",
    "                [0,0,0,0,0,0,1,1,0],\n",
    "                [0,0,0,0,0,0,1,0,1],\n",
    "                [0,0,0,0,0,0,0,1,1],\n",
    "                [1,0,0,0,0,0,0,1,1],\n",
    "                [1,0,0,0,0,0,1,0,1],\n",
    "                [1,0,0,0,0,0,1,1,1],\n",
    "                [0,0,0,0,0,1,0,0,0],\n",
    "                [1,0,0,0,0,1,0,0,0],\n",
    "                [0,0,0,0,0,1,0,0,1],\n",
    "                [0,0,0,0,0,0,1,1,0]]\n",
    "\n",
    "\n",
    "env = retro.make(game='SuperMarioBros-Nes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN():\n",
    "  def __init__(self):\n",
    "    num_actions = 18\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(4,200,200))\n",
    "    preprocessor = layers.experimental.preprocessing.Resizing(200, 200, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(preprocessor)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "def preprocess(image):\n",
    "    output = np.average(np.array(image), axis=2)[25:205]\n",
    "    #return output\n",
    "    return cv2.resize(output, dsize=(200, 200), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def popback(state_block, incoming_state):\n",
    "    state_block.pop(0)\n",
    "    state_block.append(incoming_state)\n",
    "    return state_block\n",
    "\n",
    "def gradient_update(state_history, \n",
    "                    next_state_history,\n",
    "                    rewards_history,\n",
    "                    action_history,\n",
    "                    loss_history,\n",
    "                    model, \n",
    "                    gamma,\n",
    "                    batch_size):\n",
    "    \n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            next_state_sample = np.array([next_state_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices])\n",
    "            print ('Memory contains ', len(action_history), 'states.')\n",
    "            future_rewards = model.toymodel.predict((np.array(next_state_sample)))\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            masks = tf.one_hot(action_sample, 18)\n",
    "            with tf.GradientTape() as tape:  \n",
    "                q_values = model.forward((np.array(state_sample)))\n",
    "                q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = model.loss_fn(updated_q_values, q_actions)\n",
    "            loss_history.append(loss)\n",
    "            grads = tape.gradient(loss, model.toymodel.trainable_variables)\n",
    "            print ('Gradient updated. Loss at',float(loss))\n",
    "            model.toymodel.optimizer.apply_gradients(zip(grads, model.toymodel.trainable_variables))\n",
    "            model.toymodel.save('120222_SMB')\n",
    "                                         \n",
    "def memory_manager(array, memory_length):\n",
    "    excess = len(array)-memory_length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = ANN()\n",
    "target = ANN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0\n",
    "min_epsilon = 0\n",
    "gamma = 0.99\n",
    "max_memory_len = 100000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "action_history = []\n",
    "state_history= []\n",
    "next_state_history = []\n",
    "reward_history = []\n",
    "done_history = []\n",
    "episodic_return = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient updated. Loss at 0.8302001953125\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n",
      "Gradient updated. Loss at 2924.88916015625\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n",
      "Gradient updated. Loss at 3.39141845703125\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n",
      "Gradient updated. Loss at 1316.4393310546875\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n",
      "Gradient updated. Loss at 228.285888671875\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n",
      "Gradient updated. Loss at 127.03514862060547\n",
      "INFO:tensorflow:Assets written to: 120222_SMB\\assets\n"
     ]
    }
   ],
   "source": [
    "for episodes in range(1000):\n",
    "    s = []\n",
    "    s.append(preprocess(env.reset()))\n",
    "    #Prime the state s with 3 frames.\n",
    "    prelim_reward = 0\n",
    "    epi_return = 0 \n",
    "    for i in range(3):\n",
    "        frame, reward, done, info = env.step(action_space[0])\n",
    "        prelim_reward += reward\n",
    "        s.append(preprocess(frame))\n",
    "    done = False\n",
    "    #Choose an initial action.\n",
    "    if np.random.random() < epsilon:\n",
    "        a = np.random.choice(np.arange(len(action_space)))\n",
    "    else:\n",
    "        a = np.argmax(behavior.forward(np.expand_dims(s,0)))\n",
    "    while not done:\n",
    "        new_frame, reward, done, info = env.step(action_space[a])\n",
    "        s_prime = popback(s, preprocess(new_frame))\n",
    "        epi_return += reward\n",
    "        #env.render()\n",
    "        if done:\n",
    "            #Add gradient minimization step here.\n",
    "            return_history.append(epi_return+prelim_reward)\n",
    "            break\n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.choice(np.arange(len(action_space)))\n",
    "        else:\n",
    "            a_prime = np.argmax(behavior.forward(np.expand_dims(s,0)))\n",
    "        #Save to history\n",
    "        reward_history.append(prelim_reward)\n",
    "        state_history.append(s)\n",
    "        action_history.append(a)\n",
    "        next_state_history.append(s_prime)\n",
    "        done_history.append(done)\n",
    "        if len(reward_history)>32:\n",
    "            gradient_update(state_history, \n",
    "                            next_state_history,\n",
    "                            reward_history,\n",
    "                            action_history,\n",
    "                            loss_history,\n",
    "                            behavior, \n",
    "                            gamma,\n",
    "                            batch_size)\n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(np.arange(len(episodic_return)), episodic_return)\n",
    "    plt.ylabel('Return')\n",
    "    plt.xlabel('Episode')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
