{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import logging\n",
    "#import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import keyboard\n",
    "import gym\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import clear_output\n",
    "#retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs, rew, done, info = env.step(env.action_space.sample())\n",
    "#(224, 240, 3)\n",
    "#MultiBinary(9)                          \n",
    "class RAM_ANN():\n",
    "  def __init__(self, action_space, state_space, frameskip, seed):\n",
    "    self.seed = seed\n",
    "    initializer1 = initializers.GlorotUniform (seed = self.seed+1)\n",
    "    initializer2 = initializers.GlorotUniform (seed = self.seed+2)\n",
    "    initializer3 = initializers.GlorotUniform (seed = self.seed+3)\n",
    "    initializer4 = initializers.GlorotUniform (seed = self.seed+4)\n",
    "    self.frameskip = frameskip\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    ram_input = layers.Input(shape=(state_space))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    \n",
    "    layer5 = layers.Dense(128, activation=\"relu\", kernel_initializer = initializer1)(ram_input)\n",
    "    layer6 = layers.Dense(512, activation=\"relu\", kernel_initializer = initializer2)(layer5)\n",
    "    layer7 = layers.Dense(128, activation=\"relu\", kernel_initializer = initializer3)(layer6)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\", kernel_initializer = initializer4)(layer7)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=ram_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "class A3C():\n",
    "  def __init__(self, action_space, state_space, frameskip, seed):\n",
    "    self.seed = seed\n",
    "    initializer1 = initializers.GlorotUniform (seed = self.seed+1)\n",
    "    initializer2 = initializers.GlorotUniform (seed = self.seed+2)\n",
    "    initializer3 = initializers.GlorotUniform (seed = self.seed+3)\n",
    "    initializer4 = initializers.GlorotUniform (seed = self.seed+4)\n",
    "    self.frameskip = frameskip\n",
    "    self.action_space = len(action_space)\n",
    "    self.num_actions = self.action_space\n",
    "    self.state_space = state_space\n",
    "    #NN layers\n",
    "    ram_input = layers.Input(shape=(self.state_space))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer5 = layers.Dense(256, activation=\"relu\", kernel_initializer = initializer1)(ram_input)\n",
    "    logaction = layers.Dense(self.num_actions, activation=\"linear\", kernel_initializer = initializer2)(layer5)\n",
    "    value = layers.Dense(1, activation=\"linear\", kernel_initializer = initializer3)(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=ram_input, outputs=[logaction, value])\n",
    "    self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "\n",
    "class CONV_ANN():\n",
    "  def __init__(self, action_space, ndim):\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(ndim,ndim,4))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0000625, epsilon=0.00015)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "class LookAhead():\n",
    "  def __init__(self, action_space):\n",
    "    self.action_space = len(action_space)\n",
    "    image_input = layers.Input(shape=(84,84,4))\n",
    "    action_input = layers.Input(shape=self.action_space)\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Concatenate(axis=1)([layer4, action_input])\n",
    "    layer6 = layers.Dense(512, activation=\"relu\")(layer5)\n",
    "    layer7 = layers.Dense(3136, activation=\"relu\")(layer6)\n",
    "    layer8 = layers.Reshape((7, 7, 64))(layer7)\n",
    "    layer9 = layers.Conv2DTranspose(64,3)(layer8)\n",
    "    layer10 = layers.Conv2DTranspose(32,12)(layer9)\n",
    "    layer11 = layers.Conv2DTranspose(4,65)(layer10)\n",
    "    value_output = layers.Dense(1, activation='linear')(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    toymodel = keras.Model(inputs=[image_input, action_input], outputs=[layer11, value_output])\n",
    "    loss_fn = tf.keras.losses.Huber()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.00025)\n",
    "    toymodel.compile(optimizer, loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "class Agent():\n",
    "    def __init__(self,runname, action_space, num_of_threads = 1, rom_name = 'CartPole-v1'):\n",
    "        self.rom_name = rom_name\n",
    "        self.eval_env = gym.make(self.rom_name)\n",
    "        self.action_space = action_space\n",
    "        self.num_of_threads = int(num_of_threads)\n",
    "        # Every number of steps equal to the epoch length, evalulation a greedy run of the Q function.\n",
    "        self.eval_epsilon = 0.05\n",
    "        self.eval_reps = 10\n",
    "        self.epoch_len = 50000\n",
    "        \n",
    "        self.steps_taken = 0 \n",
    "        self.runname = runname\n",
    "        self.len_of_episode = 10000\n",
    "\n",
    "        #Set hyperparameters.\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_min = 0\n",
    "        self.epsilon_lag = 50000\n",
    "        self.annealing_time = 1000000\n",
    "        self.gamma = 0.99\n",
    "        self.max_memory_len = 1000000\n",
    "        self.batch_size = 32\n",
    "        self.steps_per_update = 4\n",
    "        self.reward_scaler = 0.01\n",
    "        self.target_update = 10000\n",
    "        self.window = 25\n",
    "        self.frameskip = 8 #Deprecated for the moment and needs updating\n",
    "        \n",
    "        #Initialize containers which will be prepared in thread_prep()\n",
    "        self.loss_history = []\n",
    "        self.action_history = []\n",
    "        self.state_history= []\n",
    "        self.next_state_history = []\n",
    "        self.reward_history = []\n",
    "        self.done_history = []\n",
    "        self.episodic_return = []\n",
    "        self.return_history = [] \n",
    "        self.env_container = []\n",
    "        self.threads = []\n",
    "        self.epsilon_schedule = []\n",
    "        self.grads = []\n",
    "        self.evaluations = []\n",
    "        self.thread_prep()\n",
    "        \n",
    "        #Initialize target and behavior network.\n",
    "        self.seed = 42\n",
    "        self.behavior = CONV_ANN(self.action_space, 84) #RAM_ANN(self.action_space, 128, self.frameskip, self.seed)\n",
    "        self.target = CONV_ANN(self.action_space, 84) #RAM_ANN(self.action_space, 128, self.frameskip,self.seed)\n",
    "        self.A3C = A3C(self.action_space, 128, self.frameskip, self.seed)\n",
    "\n",
    "    def clip_reward(self, reward):\n",
    "        if reward > 0:\n",
    "            return 1\n",
    "        elif reward == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def popback(self, state_block, incoming_state):\n",
    "        state_block.pop(0)\n",
    "        state_block.append(incoming_state)\n",
    "        return state_block\n",
    "\n",
    "    def gradient_update(self, \n",
    "                        runname,\n",
    "                        state_history, \n",
    "                        next_state_history,\n",
    "                        rewards_history,\n",
    "                        action_history,\n",
    "                        loss_history,\n",
    "                        model,\n",
    "                        target_model,\n",
    "                        gamma,\n",
    "                        batch_size,\n",
    "                        done_history,\n",
    "                        action_space):\n",
    "    \n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            next_state_sample = np.array([next_state_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "            future_rewards = target_model.toymodel.predict(next_state_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards)\n",
    "            updated_q_values = updated_q_values *(1-done_sample) - done_sample\n",
    "            masks = tf.one_hot(action_sample, len(action_space))\n",
    "            with tf.GradientTape() as tape:  \n",
    "                q_values = model.toymodel(state_sample)\n",
    "                q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = model.loss_fn(updated_q_values, q_actions)\n",
    "            loss_history = loss_history.append(loss)\n",
    "            grads = tape.gradient(loss, model.toymodel.trainable_variables)\n",
    "            model.toymodel.optimizer.apply_gradients(zip(grads, model.toymodel.trainable_variables))\n",
    "                      \n",
    "    def save_history(self,):   \n",
    "        runname = self.runname\n",
    "        np.save(runname + 'action_history',self.action_history)\n",
    "        np.save(runname + 'state_history', self.state_history)\n",
    "        np.save(runname + 'next_state_history', self.next_state_history)\n",
    "        np.save(runname + 'reward_history', self.reward_history)\n",
    "        np.save(runname + 'done_history', self.done_history)\n",
    "        np.save(runname + 'return_history', self.episodic_return)\n",
    "        np.save(runname + 'evaluations', self.evaluations)\n",
    "        np.save(runname + 'loss_history', self.loss_history)\n",
    "        self.behavior.toymodel.save(runname+'_behavior')\n",
    "        self.target.toymodel.save(runname+'_target')\n",
    "\n",
    "    def RGB_preprocess(self, action, env, frameskips):\n",
    "        state_output = []\n",
    "        reward = 0\n",
    "        for i in range(frameskips):\n",
    "            s, r, d, info = env.step(action)\n",
    "            reward += r\n",
    "            s = cv2.cvtColor(s, cv2.COLOR_RGB2GRAY)\n",
    "            s = cv2.resize(s, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "            state_output.append(s/255.0)\n",
    "        #return np.max(np.dstack(state_output), axis=2), reward, d, info #For max stacking\n",
    "        return np.dstack(state_output).astype('float16'), reward, d, info #For frame stacking\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        #return [state[0]/4.8, state[1], state[2]/0.418, state[3]]\n",
    "        return state/255.0\n",
    "\n",
    "    def memory_manager(self,array, mem_size):\n",
    "        num_delete = len(array) - mem_size\n",
    "        if num_delete < 0:\n",
    "            None\n",
    "        else:\n",
    "            del array[:num_delete]\n",
    "            \n",
    "    def piecewise_epsilon(self, steps_taken, lag, annealingtime, ep_min, ep_max): #returns epsilon\n",
    "        anneal_slope= (ep_min-ep_max)/(lag+annealingtime-lag)\n",
    "        if steps_taken < lag: return ep_max\n",
    "        if (steps_taken >= lag) and (steps_taken < (lag+annealingtime)): return anneal_slope*steps_taken+(ep_max-anneal_slope*lag)\n",
    "        else: return ep_min\n",
    "\n",
    "    def sliding_average(self, array, n):\n",
    "        output = []\n",
    "        for i in range(len(array)):\n",
    "            try:\n",
    "                output.append(np.average(array[i:i+n]))\n",
    "            except IndexError:\n",
    "                break\n",
    "        return output\n",
    "    \n",
    "    def thread_prep(self):\n",
    "        self.loss_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.action_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.state_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.next_state_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.reward_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.done_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.episodic_return = [[] for i in range(self.num_of_threads)]\n",
    "        self.return_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.env_container = [gym.make(self.rom_name) for i in range(self.num_of_threads)]\n",
    "        self.steps_taken = [0 for i in range(self.num_of_threads)]\n",
    "        self.evaluations = [[] for i in range(self.num_of_threads)]\n",
    "        self.epsilon_schedule = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        \n",
    "        \n",
    "    def agent_thread(self, thread_num, num_training_steps, seed=42):\n",
    "        logging.info(\"Thread %s: starting\", thread_num)\n",
    "        env = self.env_container[thread_num]\n",
    "        #self.env_container[thread_num].seed(seed)\n",
    "        self.RGB_episode(num_training_steps, env, thread_num, epsilon)\n",
    "        self.episode(num_training_steps*self.steps_per_update, self.env_container[thread_num], thread_num, self.epsilon_schedule[thread_num])\n",
    "        logging.info(\"Thread %s: finishing\", thread_num)\n",
    "     \n",
    "    def start_threads(self, num_training_steps=1):\n",
    "        format = \"%(asctime)s: %(message)s\"\n",
    "        logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "        grads = []\n",
    "        for i in range(self.num_of_threads):\n",
    "            logging.info(\"Main    : create and start thread %d.\", i)\n",
    "            x = threading.Thread(target=self.agent_thread, args=(i, num_training_steps))\n",
    "            self.threads.append(x)\n",
    "            x.start()\n",
    "                \n",
    "        while len(self.state_history[0]) < num_training_steps:\n",
    "            self.plot_data()\n",
    "            time.sleep(10)\n",
    "                            \n",
    "        for j in self.threads:\n",
    "            j.join()\n",
    "\n",
    "        self.plot_data()\n",
    "        \n",
    "    def plot_data(self,):\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.episodic_return[i], label='Thread ' + str(i))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Return')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.sliding_average(self.episodic_return[i], self.window), label='Thread '+str(i))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(str(self.window)+'-averaged Return')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.evaluations[i], label='Thread '+str(i))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Evaluation Return')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.loss_history[i], label='Thread '+str(i))\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            Y = [self.piecewise_epsilon(i, self.epsilon_lag, self.annealing_time, self.epsilon_min, self.epsilon_max) for i in np.arange(self.steps_taken[i])]\n",
    "            plt.plot(Y, label='Thread '+str(i))\n",
    "        plt.xlabel('Steps taken')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "        \n",
    "    def evaluate_NN(self, env, epsilon, replicates, thread_num):\n",
    "        av_returns = []\n",
    "        for i in range(replicates):\n",
    "            s = env.reset()\n",
    "            s, episode_return, done, info = self.RGB_preprocess(1, env, self.frameskip)\n",
    "            episode_return = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                if np.random.random() < epsilon:\n",
    "                    a = np.random.choice(np.arange(len(self.action_space)))\n",
    "                else: \n",
    "                    a_probs = self.behavior.toymodel(np.expand_dims(s,0), training=False)\n",
    "                    a = tf.argmax(a_probs[0]).numpy()\n",
    "                s, reward, done, info = self.RGB_preprocess(a, env, self.frameskip)\n",
    "                episode_return += reward\n",
    "            av_returns.append(episode_return)\n",
    "        self.evaluations[thread_num].append(np.average(av_returns))\n",
    "        \n",
    "    def RGB_episode(self, num_training_steps, env, thread_num, epsilon):    #Double Deep Q\n",
    "        #np.random.seed(self.seed)\n",
    "        epsilon = self.piecewise_epsilon(self.steps_taken[thread_num], self.epsilon_lag, self.annealing_time, self.epsilon_min, self.epsilon_max)\n",
    "        while self.steps_taken[thread_num] < num_training_steps:\n",
    "            env.seed(self.seed)\n",
    "            if len(self.episodic_return[thread_num])%25==0: self.plot_data()\n",
    "            epi_return = 0 \n",
    "            lives = 5     #5 for Breakout, 3 for Space Invaders\n",
    "            env.reset()\n",
    "            s, reward, done, info = self.RGB_preprocess(1, env, self.frameskip)\n",
    "            #Enter the loop.\n",
    "            while self.steps_taken[thread_num] < num_training_steps:\n",
    "                if self.steps_taken[thread_num]%self.epoch_len==0: \n",
    "                    self.evaluate_NN(self.eval_env, self.eval_epsilon, self.eval_reps, thread_num)\n",
    "                    self.save_history()\n",
    "                #Break the loop if the maximum number of training examples have been reached.\n",
    "                if self.steps_taken[thread_num] >= num_training_steps:\n",
    "                    break\n",
    "                #Choose an action from according to epsilson-greedy policy.  \n",
    "                if np.random.random() < epsilon:\n",
    "                    a = np.random.choice(np.arange(len(self.action_space)))\n",
    "                else: \n",
    "                    a_probs = self.behavior.toymodel(np.expand_dims(s,0), training=False)\n",
    "                    a = tf.argmax(a_probs[0]).numpy()\n",
    "                s_prime, reward, done, info = self.RGB_preprocess(self.action_space[a], env, self.frameskip)\n",
    "                epi_return += reward\n",
    "              \n",
    "                #Restart when the end of the episode is reached.  \n",
    "                if done:                                                                              #FUNCTIONIZE!\n",
    "                    #Set the last frame to -1 to discourage dying.                                             \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    #self.reward_history[thread_num][-1] = -1\n",
    "                    break\n",
    "                if not (int(info['ale.lives']) == lives):                                         \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    #self.reward_history[thread_num][-1] = -1\n",
    "                    lives = int(info['ale.lives'])\n",
    "                    #print ('Episode finished in ', step_in_episode, 'steps.')\n",
    "                    break                                  \n",
    "                #Monitor the the number of lives from the environemtnt. If the number of lives is reduced, then the player has died. Reset the level.  FUNCTIONIZE!                \n",
    "\n",
    "                #Save to history\n",
    "                self.reward_history[thread_num].append(self.clip_reward(reward)*self.reward_scaler)\n",
    "                self.state_history[thread_num].append(s)\n",
    "                self.action_history[thread_num].append(a)\n",
    "                self.next_state_history[thread_num].append(s_prime)\n",
    "                self.done_history[thread_num].append(done)\n",
    "                 \n",
    "                if self.steps_taken[thread_num]>self.batch_size and self.steps_taken[thread_num]%self.steps_per_update==0:\n",
    "                      self.gradient_update(self.runname,\n",
    "                                        self.state_history[thread_num], \n",
    "                                        self.next_state_history[thread_num],\n",
    "                                        self.reward_history[thread_num],\n",
    "                                        self.action_history[thread_num],\n",
    "                                        self.loss_history[thread_num],\n",
    "                                        self.behavior,\n",
    "                                        self.target,\n",
    "                                        self.gamma,\n",
    "                                        self.batch_size,\n",
    "                                        self.done_history[thread_num],\n",
    "                                        self.action_space) \n",
    "                        \n",
    "                        \n",
    "                if self.steps_taken[thread_num]%self.target_update==0:\n",
    "                    self.target.toymodel.set_weights(self.behavior.toymodel.get_weights()) \n",
    "\n",
    "                s = s_prime\n",
    "\n",
    "                self.steps_taken[thread_num] += 1\n",
    "                self.memory_manager(self.action_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.next_state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.reward_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.done_history[thread_num], self.max_memory_len)\n",
    "            self.episodic_return[thread_num].append(epi_return)\n",
    "        env.close()\n",
    "        #self.behavior.toymodel.save('120228_Breakout')\n",
    "                               \n",
    "    def A3C_episode(self, num_training_steps, env, thread_num):    #Double Deep Q\n",
    "        #np.random.seed(self.seed)\n",
    "        while self.steps_taken[thread_num] < num_training_steps:\n",
    "            env.seed(self.seed)\n",
    "            epi_return = 0 \n",
    "            steps = 0\n",
    "            lives = 5     #5 for Breakout, 3 for Space Invaders\n",
    "            s = self.preprocess(env.reset())\n",
    "            s, reward, done, info = env.step(1)\n",
    "            done = False\n",
    "            #Enter the loop.\n",
    "            if len(self.episodic_return[thread_num])%25==0: self.plot_data()\n",
    "            while self.steps_taken[thread_num] < num_training_steps:\n",
    "\n",
    "                #Break the loop if the maximum number of training examples have been reached.\n",
    "                if self.steps_taken[thread_num] >= num_training_steps:\n",
    "                    break\n",
    "                #Choose an action from according to epsilson-greedy policy.  \n",
    "                a_probs, value = self.A3C.toymodel(np.expand_dims(s,0), training=False)\n",
    "                a = np.random.choice(self.action_space,p=tf.nn.softmax(a_probs[0]).numpy())\n",
    "                \n",
    "                s_prime, reward, done, info = env.step(self.action_space[a])\n",
    "                s_prime = self.preprocess(s_prime)\n",
    "                epi_return += reward\n",
    "\n",
    "                #Restart when the end of the episode is reached.  \n",
    "                if done:                                                                              #FUNCTIONIZE!\n",
    "                    #Set the last frame to -1 to discourage dying.                                             \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    self.reward_history[thread_num][-1] = -1\n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    break\n",
    "                if not (int(info['ale.lives']) == lives):                                         \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    self.reward_history[thread_num][-1] = -1\n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    lives = int(info['ale.lives'])\n",
    "                    #print ('Episode finished in ', step_in_episode, 'steps.')\n",
    "                    break               \n",
    "                                \n",
    "                #Monitor the the number of lives from the environemtnt. If the number of lives is reduced, then the player has died. Reset the level.  FUNCTIONIZE!                \n",
    "\n",
    "                #Save to history\n",
    "                self.reward_history[thread_num].append(reward*self.reward_scaler)\n",
    "                self.state_history[thread_num].append(s)\n",
    "                self.action_history[thread_num].append(a)\n",
    "                self.next_state_history[thread_num].append(s_prime)\n",
    "                self.done_history[thread_num].append(done)\n",
    "                 \n",
    "                if self.steps_taken[thread_num]>self.batch_size and self.steps_taken[thread_num]%self.steps_per_update==0:\n",
    "                    #Enable option for experience replay by random sampling or total memory sampling.\n",
    "                    # Get indices of samples for replay buffers\n",
    "                    indices = np.random.choice(range(len(self.done_history[thread_num])), size=self.batch_size)\n",
    "                    # Using list comprehension to sample from replay buffer\n",
    "                    state_sample = np.array([self.state_history[thread_num][i] for i in indices])\n",
    "                    next_state_sample = np.array([self.next_state_history[thread_num][i] for i in indices])\n",
    "                    rewards_sample = [self.reward_history[thread_num][i] for i in indices]\n",
    "                    action_sample = [self.action_history[thread_num][i] for i in indices]\n",
    "                    done_sample = tf.convert_to_tensor([float(self.done_history[thread_num][i]) for i in indices])\n",
    "                                        \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        future_rewards = self.A3C.toymodel(next_state_sample)[1][:,0]\n",
    "                        future_rewards = future_rewards *(1-done_sample) - done_sample                    \n",
    "                        discounted_reward = rewards_sample + self.gamma * future_rewards   \n",
    "                        logits, values = self.A3C.toymodel(state_sample)   \n",
    "                        values = values[:,0]\n",
    "                        advantage = discounted_reward-values\n",
    "                        masks = tf.one_hot(action_sample, len(self.action_space))\n",
    "                        policy = tf.nn.softmax(logits)\n",
    "                        entropy = tf.reduce_sum(policy * tf.math.log(policy+1e-20),axis=1)\n",
    "                        value_loss = advantage**2\n",
    "                        policy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=masks, logits=logits)\n",
    "                        policy_loss *= tf.stop_gradient(advantage)\n",
    "                        policy_loss -= 0.0005 * entropy\n",
    "                        total_loss = (0.5 * value_loss + policy_loss)\n",
    "                    loss_history = self.loss_history[thread_num].append(np.mean(total_loss))\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(total_loss, self.A3C.toymodel.trainable_weights)\n",
    "                    # Push local gradients to global model\n",
    "                    self.A3C.toymodel.optimizer.apply_gradients(zip(grads, self.A3C.toymodel.trainable_weights))\n",
    "                    steps += 1                 \n",
    "                s = s_prime\n",
    "\n",
    "                self.steps_taken[thread_num] += 1\n",
    "                self.memory_manager(self.action_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.next_state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.reward_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.done_history[thread_num], self.max_memory_len)          \n",
    "        env.close()\n",
    "\n",
    "\n",
    "        #self.behavior.toymodel.save('120228_Breakout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_action_space = np.arange(4)\n",
    "env = gym.make(\"BreakoutDeterministic-v4\")\n",
    "agent = Agent('210405_1', atari_action_space, num_of_threads = 1, rom_name=\"BreakoutDeterministic-v4\")\n",
    "agent.epsilon = 1.0\n",
    "agent.epsilon_max = 1.0\n",
    "agent.epsilon_min = 0.1\n",
    "agent.epsilon_lag = 10000\n",
    "agent.annealing_time = 1000000\n",
    "agent.len_of_episode = 10000\n",
    "agent.gamma = 0.99\n",
    "agent.max_memory_len = 100000\n",
    "agent.batch_size = 32\n",
    "agent.steps_per_update = 4\n",
    "agent.reward_scaler = 1\n",
    "agent.target_update = 50000\n",
    "agent.epsilon_schedule = [0.05, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "agent.frameskip = 4\n",
    "\n",
    "#Evaluation parms\n",
    "agent.eval_epsilon = 0.05\n",
    "agent.eval_reps = 10\n",
    "agent.epoch_len = 5000\n",
    "#agent.episode(10000000, env1)\n",
    "#LR is 0.0000625\n",
    "agent.behavior.toymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.RGB_episode(100000000, env, 0, 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
