{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import cv2\n",
    "#retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs, rew, done, info = env.step(env.action_space.sample())\n",
    "#(224, 240, 3)\n",
    "#MultiBinary(9)\n",
    "#[\"B\", null, \"SELECT\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\"]\n",
    "action_space = [[0,0,0,0,0,0,0,0,0],\n",
    "                [1,0,0,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,0,0,1],\n",
    "                [1,0,0,0,0,0,1,0,0],\n",
    "                [1,0,0,0,0,0,0,1,0],\n",
    "                [1,0,0,0,0,0,0,0,1],\n",
    "                [0,0,0,0,0,0,1,1,0],\n",
    "                [0,0,0,0,0,0,1,0,1],\n",
    "                [0,0,0,0,0,0,0,1,1],\n",
    "                [1,0,0,0,0,0,0,1,1],\n",
    "                [1,0,0,0,0,0,1,0,1],\n",
    "                [1,0,0,0,0,0,1,1,1],\n",
    "                [0,0,0,0,0,1,0,0,0],\n",
    "                [1,0,0,0,0,1,0,0,0],\n",
    "                [0,0,0,0,0,1,0,0,1],\n",
    "                [0,0,0,0,0,0,1,1,0]]\n",
    "\n",
    "class ANN():\n",
    "  def __init__(self):\n",
    "    num_actions = 18\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(4,84,84,))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 4, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 1, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 1, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "def preprocess(image):\n",
    "    output = np.average(np.array(image), axis=2)[25:205]\n",
    "    #return output\n",
    "    return cv2.resize(output, dsize=(84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "def popback(state_block, incoming_state):\n",
    "    state_block.pop(0)\n",
    "    state_block.append(incoming_state)\n",
    "    return state_block\n",
    "\n",
    "def gradient_update(state_history, \n",
    "                    next_state_history,\n",
    "                    rewards_history,\n",
    "                    action_history,\n",
    "                    loss_history,\n",
    "                    model,\n",
    "                    target_model,\n",
    "                    gamma,\n",
    "                    batch_size):\n",
    "    \n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            next_state_sample = np.array([next_state_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices])\n",
    "            print ('Memory contains ', len(action_history), 'states.')\n",
    "            future_rewards = target_model.toymodel.predict((np.array(next_state_sample)))\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            masks = tf.one_hot(action_sample, 18)\n",
    "            with tf.GradientTape() as tape:  \n",
    "                q_values = model.forward((np.array(state_sample)))\n",
    "                q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = model.loss_fn(updated_q_values, q_actions)\n",
    "            loss_history.append(loss)\n",
    "            grads = tape.gradient(loss, model.toymodel.trainable_variables)\n",
    "            print ('Gradient updated. Loss at',float(loss))\n",
    "            model.toymodel.optimizer.apply_gradients(zip(grads, model.toymodel.trainable_variables))\n",
    "            model.toymodel.save('120227_SMB')\n",
    "            np.save(runname + 'loss_function_history', loss_history)\n",
    "                                         \n",
    "   \n",
    "behavior = ANN()\n",
    "target = ANN()\n",
    "behavior.toymodel = keras.models.load_model('120223_SMB')\n",
    "env = retro.make(game='SuperMarioBros-Nes')\n",
    "\n",
    "runname = \"210223_1\"\n",
    "\n",
    "epsilon = 0.05\n",
    "epsilon_min = 0.05\n",
    "gamma = 0.99\n",
    "max_memory_len = 100000\n",
    "batch_size = 32\n",
    "\n",
    "loss_history = []\n",
    "action_history = []\n",
    "state_history= []\n",
    "next_state_history = []\n",
    "reward_history = []\n",
    "done_history = []\n",
    "episodic_return = []\n",
    "return_history = []\n",
    "step_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ce4357b67db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Prime the state s with 3 frames.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprelim_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "for episodes in range(1000):\n",
    "    s = []\n",
    "    s.append(preprocess(env.reset()))\n",
    "    #Prime the state s with 3 frames.\n",
    "    prelim_reward = 0\n",
    "    epi_return = 0 \n",
    "    for i in range(3):\n",
    "        frame, reward, done, info = env.step(action_space[0])\n",
    "        prelim_reward += reward\n",
    "        s.append(preprocess(frame))\n",
    "    done = False\n",
    "    #Choose an initial action.\n",
    "    if np.random.random() < np.max([epsilon,epsilon_min]):\n",
    "        a = np.random.choice(np.arange(len(action_space)))\n",
    "    else:\n",
    "        a = np.argmax(behavior.forward(np.expand_dims(s,0)))\n",
    "    while not done:\n",
    "        #env.render()\n",
    "        new_frame, reward, done, info = env.step(action_space[a])\n",
    "        s_prime = popback(s, preprocess(new_frame))\n",
    "        epi_return += reward\n",
    "        reward -= 1\n",
    "        env.render()\n",
    "        if done:\n",
    "            #Add gradient minimization step here.\n",
    "            return_history.append(epi_return+prelim_reward)\n",
    "            break\n",
    "        if np.random.random() < np.max([epsilon,epsilon_min]):\n",
    "            a_prime = np.random.choice(np.arange(len(action_space)))\n",
    "        else:\n",
    "            a_prime = np.argmax(behavior.forward(np.expand_dims(s,0)))\n",
    "        #Save to history\n",
    "        reward_history.append(prelim_reward)\n",
    "        state_history.append(s)\n",
    "        action_history.append(a)\n",
    "        next_state_history.append(s_prime)\n",
    "        done_history.append(done)\n",
    "        if len(reward_history)>32 and step_counter%4==0:\n",
    "            gradient_update(state_history, \n",
    "                            next_state_history,\n",
    "                            reward_history,\n",
    "                            action_history,\n",
    "                            loss_history,\n",
    "                            behavior, \n",
    "                            target,\n",
    "                            gamma,\n",
    "                            batch_size)\n",
    "            step_counter += 1\n",
    "        if step_counter%10000==0:\n",
    "            target.toymodel.set_weights(behavior.toymodel.get_weights()) \n",
    "        step_counter += 1    \n",
    "        s = s_prime\n",
    "        a = a_prime\n",
    "        epsilon -= 0.00009\n",
    "\n",
    "        if len(reward_history)>1000000:\n",
    "            action_history.pop(0)\n",
    "            state_history.pop(0)\n",
    "            next_state_history.pop(0)\n",
    "            reward_history.pop(0)\n",
    "            done_history.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
