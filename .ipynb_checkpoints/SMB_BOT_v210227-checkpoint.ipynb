{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import cv2\n",
    "#import keyboard\n",
    "import gym\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import clear_output\n",
    "#retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs, rew, done, info = env.step(env.action_space.sample())\n",
    "#(224, 240, 3)\n",
    "#MultiBinary(9)                          \n",
    "\n",
    "class ANN():\n",
    "  def __init__(self, action_space):\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(84,84,4))\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "class Atari_ANN():\n",
    "  def __init__(self, action_space):\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(84,84,4))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=8, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "def detect_input():\n",
    "    #[\"B\", null, \"SELECT\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\"]\n",
    "    output = [0,0,0,0,0,0,0,0,0]\n",
    "    print ('Waiting for human input...')\n",
    "    while True:\n",
    "        if keyboard.is_pressed('/'):\n",
    "            if keyboard.is_pressed('w'): output[4] = 1\n",
    "            if keyboard.is_pressed('s'): output[5] = 1\n",
    "            if keyboard.is_pressed('a'): output[6] = 1\n",
    "            if keyboard.is_pressed('d'): output[7] = 1\n",
    "            if keyboard.is_pressed('v'): output[8] = 1\n",
    "            if keyboard.is_pressed('b'): output[0] = 1\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "    print (output)\n",
    "    time.sleep(0.1)\n",
    "    return output\n",
    "\n",
    "def load_history(action_history, state_history, next_state_history, reward_history, done_history, return_history):\n",
    "    action_history = np.load(runname + 'action_history' + '.npy')\n",
    "    state_history = np.load(runname + 'state_history' + '.npy')\n",
    "    #next_state_history = np.load(runname + 'next_state_history' + '.npy')\n",
    "    #reward_history = np.load(runname + 'reward_history' + '.npy')\n",
    "    #done_history = np.load(runname + 'done_history' + '.npy')\n",
    "    #return_history = np.load(runname + 'return_history' + '.npy') \n",
    "\n",
    "\n",
    "#behavior.toymodel = keras.models.load_model('120227_SMB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent_4Frame():\n",
    "    def __init__(self,runname, env, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.env = env\n",
    "        self.steps_taken = 0 \n",
    "        self.runname = runname\n",
    "        self.epsilon = 0\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_min = 0.1\n",
    "        self.annealing_time = 2000000\n",
    "        self.len_of_episode = 10000\n",
    "        self.gamma = 0.99\n",
    "        self.max_memory_len = 100000\n",
    "        self.batch_size = 32\n",
    "        self.loss_history = []\n",
    "        self.action_history = []\n",
    "        self.state_history= []\n",
    "        self.next_state_history = []\n",
    "        self.reward_history = []\n",
    "        self.done_history = []\n",
    "        self.episodic_return = []\n",
    "        self.return_history = [] \n",
    "        self.behavior = ANN(self.action_space)\n",
    "        self.target = ANN(self.action_space)\n",
    "    \n",
    "        #[\"B\", null, \"SELECT\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\"]\n",
    "\n",
    "\n",
    "    def preprocess(self, image):\n",
    "        output = np.average(np.array(image), axis=2)[25:205]\n",
    "        #return output\n",
    "        return cv2.resize(output, dsize=(84, 84), interpolation=cv2.INTER_CUBIC)/255.0\n",
    "\n",
    "    def popback(self, state_block, incoming_state):\n",
    "        state_block.pop(0)\n",
    "        state_block.append(incoming_state)\n",
    "        return state_block\n",
    "\n",
    "    def gradient_update(self, \n",
    "                        runname,\n",
    "                        state_history, \n",
    "                        next_state_history,\n",
    "                        rewards_history,\n",
    "                        action_history,\n",
    "                        loss_history,\n",
    "                        model,\n",
    "                        target_model,\n",
    "                        gamma,\n",
    "                        batch_size,\n",
    "                        done_history,\n",
    "                        action_space):\n",
    "    \n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            next_state_sample = np.array([next_state_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices])\n",
    "            future_rewards = target_model.toymodel.predict(next_state_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "      \n",
    "            updated_q_values = updated_q_values *(1-done_sample) - done_sample  \n",
    "                         \n",
    "            masks = tf.one_hot(action_sample, len(action_space))\n",
    "            with tf.GradientTape() as tape:  \n",
    "                q_values = model.toymodel(state_sample)\n",
    "                q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = model.loss_fn(updated_q_values, q_actions)\n",
    "                \n",
    "            loss_history.append(loss)\n",
    "            grads = tape.gradient(loss, model.toymodel.trainable_variables)\n",
    "            model.toymodel.optimizer.apply_gradients(zip(grads, model.toymodel.trainable_variables))\n",
    "\n",
    "                        \n",
    "    def save_history(self,\n",
    "                     runname,\n",
    "                     action_history,\n",
    "                     state_history,\n",
    "                     next_state_history,\n",
    "                     reward_history,\n",
    "                     done_history,\n",
    "                     return_history):            \n",
    "        np.save(runname + 'action_history',action_history)\n",
    "        np.save(runname + 'state_history', state_history)\n",
    "        np.save(runname + 'next_state_history', next_state_history)\n",
    "        np.save(runname + 'reward_history', reward_history)\n",
    "        np.save(runname + 'done_history', done_history)\n",
    "        np.save(runname + 'return_history', return_history)   \n",
    "        \n",
    "    def memory_manager(self,array, mem_size):\n",
    "        num_delete = len(array) - mem_size\n",
    "        if num_delete < 0:\n",
    "            None\n",
    "        else:\n",
    "            del array[:num_delete]\n",
    "                \n",
    "    def episode(self,num_episodes):    #Double Deep Q\n",
    "        for i in range (num_episodes):\n",
    "            self.epsilon = self.epsilon_max - (((self.epsilon_max-self.epsilon_min)/self.annealing_time)*self.steps_taken)\n",
    "            print ('Epsilon is at ', np.max([self.epsilon, self.epsilon_min]), ' as of step ', self.steps_taken)\n",
    "            epi_return = 0 \n",
    "            done = False\n",
    "            lives = 2\n",
    "            \n",
    "            s = []\n",
    "            s.append(self.preprocess(self.env.reset()))\n",
    "            #Prime the state s with 3 frames.\n",
    "            \n",
    "            for i in range(3):\n",
    "                frame, reward, done, info = self.env.step(self.action_space[1])\n",
    "                epi_return += reward\n",
    "                s.append(self.preprocess(frame))\n",
    "\n",
    "            s_channeled = np.dstack((s[0],s[1], s[2], s[3]))\n",
    "            #Choose an initial action.\n",
    "            if np.random.random() < np.max([self.epsilon,self.epsilon_min]):\n",
    "                a = np.random.choice(np.arange(len(self.action_space)))\n",
    "            else: \n",
    "                a_probs = self.behavior.toymodel(np.expand_dims(s_channeled,0), training=False)\n",
    "                a = tf.argmax(a_probs[0]).numpy()\n",
    "        \n",
    "            while not done:\n",
    "                new_frame, reward, done, info = self.env.step(self.action_space[a])\n",
    "                s_prime = self.popback(s, self.preprocess(new_frame))      \n",
    "                s_prime_channeled = np.dstack((s_prime[0],s_prime[1], s_prime[2], s_prime[3]))\n",
    "                epi_return += reward\n",
    "                self.env.render()\n",
    "                \n",
    "                #Check if a life was lost. Is so, make the reward -1. \n",
    "                if not (lives == info['lives']):\n",
    "                    reward = -1\n",
    "                    lives = info['lives']\n",
    "                    print ('Life lost.')\n",
    "                    \n",
    "                #Check is episode is done. Assign the reward for reaching the terminal state as -1. \n",
    "                if done:\n",
    "                    self.done_history[-1] = True\n",
    "                    self.return_history.append(epi_return)\n",
    "                    break\n",
    "                if np.random.random() < np.max([self.epsilon,self.epsilon_min]):\n",
    "                    a_prime = np.random.choice(np.arange(len(self.action_space)))\n",
    "                else:\n",
    "                    a_probs = self.behavior.toymodel(np.expand_dims(s_prime_channeled,0), training=False)\n",
    "                    a_prime = tf.argmax(a_probs[0]).numpy()\n",
    "                    #print ('Nonrandom action taken. ', a_prime)\n",
    "                #Save to history\n",
    "                self.reward_history.append(reward)\n",
    "                self.state_history.append(s_channeled)\n",
    "                self.action_history.append(a)\n",
    "                self.next_state_history.append(s_prime_channeled)\n",
    "                self.done_history.append(done)\n",
    "                 \n",
    "                if len(self.reward_history)>32 and self.steps_taken%8==0:\n",
    "                    self.gradient_update(self.runname,\n",
    "                                         self.state_history, \n",
    "                                         self.next_state_history,\n",
    "                                         self.reward_history,\n",
    "                                         self.action_history,\n",
    "                                         self.loss_history,\n",
    "                                         self.behavior, \n",
    "                                         self.target,\n",
    "                                         self.gamma,\n",
    "                                         self.batch_size,\n",
    "                                         self.done_history,\n",
    "                                         self.action_space)\n",
    "                if self.steps_taken%10000==0:\n",
    "                    self.target.toymodel.set_weights(self.behavior.toymodel.get_weights())\n",
    "                    print ('Target model has been updated.')\n",
    "                s = s_prime\n",
    "                a = a_prime\n",
    "                self.steps_taken += 1\n",
    "                self.memory_manager(self.action_history, self.max_memory_len)\n",
    "                self.memory_manager(self.state_history, self.max_memory_len)\n",
    "                self.memory_manager(self.next_state_history, self.max_memory_len)\n",
    "                self.memory_manager(self.reward_history, self.max_memory_len)\n",
    "                self.memory_manager(self.done_history, self.max_memory_len)\n",
    "            #self.save_history(self.runname,\n",
    "            #                  self.action_history,\n",
    "            #                  self.state_history,\n",
    "            #                  self.next_state_history,\n",
    "            #                  self.reward_history,\n",
    "            #                  self.done_history,\n",
    "            #                  self.return_history)\n",
    "            self.episodic_return.append(epi_return)\n",
    "            print (\"Episode complete.\")\n",
    "            #self.env.close()\n",
    "            clear_output()\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.plot(np.arange(len(self.episodic_return)), self.episodic_return)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Return')\n",
    "            plt.show()\n",
    "            plt.figure(figsize=(5,5))\n",
    "            plt.plot(np.arange(len(self.loss_history)), self.loss_history)\n",
    "            plt.xlabel('Update steps')\n",
    "            plt.ylabel('Huber Loss')\n",
    "            plt.yscale('log')\n",
    "            plt.show()\n",
    "        self.behavior.toymodel.save(runname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = retro.make(game='SuperMarioBros-Nes')\n",
    "\n",
    "#[\"B\", null, \"SELECT\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\"]\n",
    "SMB_action_space = [[0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,1,0,1],\n",
    "                [0,0,0,0,0,0,0,1,1]]\n",
    "\n",
    "\n",
    "env1 = gym.make('BreakoutNoFrameskip-v4')\n",
    "atari_action_space = np.arange(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent = Agent_4Frame('210329_1', env1, atari_action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon is at  1.0  as of step  0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lives'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b636b97104bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f5249eb553d5>\u001b[0m in \u001b[0;36mepisode\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m#Check if a life was lost. Is so, make the reward -1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlives\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m                     \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                     \u001b[0mlives\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lives'"
     ]
    }
   ],
   "source": [
    "agent.episode(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.behavior.toymodel.save(agent.runname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new agent. \n",
    "agent2 = Agent_4Frame('210312_2', env, SMB_action_space)\n",
    "agent2.gamma = 0.90\n",
    "agent2.epsilon_min = 0.1\n",
    "agent.episode()\n",
    "\n",
    "agent2 = Agent_4Frame('210312_3', env, SMB_action_space)\n",
    "agent2.gamma = 0.90\n",
    "agent2.epsilon_min = 0.05\n",
    "agent.episode()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
