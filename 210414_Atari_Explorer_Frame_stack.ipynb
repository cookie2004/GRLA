{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import logging\n",
    "import retro\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.preprocessing import scale\n",
    "import numpy as np\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import keyboard\n",
    "import gym\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import clear_output\n",
    "#retro.data.list_games()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs, rew, done, info = env.step(env.action_space.sample())\n",
    "#(224, 240, 3)\n",
    "#MultiBinary(9)                          \n",
    "class RAM_ANN():\n",
    "  def __init__(self, action_space, state_space, frameskip, seed):\n",
    "    self.seed = seed\n",
    "    initializer1 = initializers.GlorotUniform (seed = self.seed+1)\n",
    "    initializer2 = initializers.GlorotUniform (seed = self.seed+2)\n",
    "    initializer3 = initializers.GlorotUniform (seed = self.seed+3)\n",
    "    initializer4 = initializers.GlorotUniform (seed = self.seed+4)\n",
    "    self.frameskip = frameskip\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    ram_input = layers.Input(shape=(state_space))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    \n",
    "    layer5 = layers.Dense(128, activation=\"relu\", kernel_initializer = initializer1)(ram_input)\n",
    "    layer6 = layers.Dense(512, activation=\"relu\", kernel_initializer = initializer2)(layer5)\n",
    "    layer7 = layers.Dense(128, activation=\"relu\", kernel_initializer = initializer3)(layer6)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\", kernel_initializer = initializer4)(layer7)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=ram_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0000625, clipnorm=1.0)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "class A3C():\n",
    "  def __init__(self, action_space, state_space, frameskip, seed):\n",
    "    self.seed = seed\n",
    "    initializer1 = initializers.GlorotUniform (seed = self.seed+1)\n",
    "    initializer2 = initializers.GlorotUniform (seed = self.seed+2)\n",
    "    initializer3 = initializers.GlorotUniform (seed = self.seed+3)\n",
    "    initializer4 = initializers.GlorotUniform (seed = self.seed+4)\n",
    "    self.frameskip = frameskip\n",
    "    self.action_space = len(action_space)\n",
    "    self.num_actions = self.action_space\n",
    "    self.state_space = state_space\n",
    "    #NN layers\n",
    "    ram_input = layers.Input(shape=(self.state_space))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer5 = layers.Dense(256, activation=\"relu\", kernel_initializer = initializer1)(ram_input)\n",
    "    logaction = layers.Dense(self.num_actions, activation=\"linear\", kernel_initializer = initializer2)(layer5)\n",
    "    value = layers.Dense(1, activation=\"linear\", kernel_initializer = initializer3)(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=ram_input, outputs=[logaction, value])\n",
    "    self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "\n",
    "class CONV_ANN():\n",
    "  def __init__(self, action_space, ndim):\n",
    "    self.action_space = len(action_space)\n",
    "    num_actions = self.action_space\n",
    "    #NN layers\n",
    "    image_input = layers.Input(shape=(ndim,ndim,4))\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    self.toymodel = keras.Model(inputs=image_input, outputs=action)\n",
    "    self.loss_fn = tf.keras.losses.Huber()\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate=0.0000625, epsilon=0.00015)\n",
    "    self.toymodel.compile(self.optimizer, self.loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)\n",
    "\n",
    "class LookAhead():\n",
    "  def __init__(self, action_space):\n",
    "    self.action_space = len(action_space)\n",
    "    image_input = layers.Input(shape=(84,84,4))\n",
    "    action_input = layers.Input(shape=self.action_space)\n",
    "    #preprocessor = layers.experimental.preprocessing.Resizing(84, 84, interpolation='bilinear', name=None)(image_input)\n",
    "    # Convolutions on the frames on the screen\n",
    "    #data_format='channels_first'\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(image_input)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "    layer5 = layers.Concatenate(axis=1)([layer4, action_input])\n",
    "    layer6 = layers.Dense(512, activation=\"relu\")(layer5)\n",
    "    layer7 = layers.Dense(3136, activation=\"relu\")(layer6)\n",
    "    layer8 = layers.Reshape((7, 7, 64))(layer7)\n",
    "    layer9 = layers.Conv2DTranspose(64,3)(layer8)\n",
    "    layer10 = layers.Conv2DTranspose(32,12)(layer9)\n",
    "    layer11 = layers.Conv2DTranspose(4,65)(layer10)\n",
    "    value_output = layers.Dense(1, activation='linear')(layer5)\n",
    "\n",
    "    #Define NN parameters.\n",
    "    toymodel = keras.Model(inputs=[image_input, action_input], outputs=[layer11, value_output])\n",
    "    loss_fn = tf.keras.losses.Huber()\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.00025)\n",
    "    toymodel.compile(optimizer, loss_fn)\n",
    "\n",
    "  def trainStep(self, sample_X, sample_Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "      old_q = self.toymodel(sample_X, training=True)\n",
    "      loss_value = self.loss_fn(sample_Y, old_q)\n",
    "    grads = tape.gradient(loss_value, self.toymodel.trainable_weights)\n",
    "    self.optimizer.apply_gradients(zip(grads, self.toymodel.trainable_weights))\n",
    "    return loss_value.numpy()\n",
    "\n",
    "  def train(self, x_input, y_input, batchsize=64):\n",
    "    loss_history = []\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_input, y_input))\n",
    "    dataset = dataset.shuffle(buffer_size=1024).batch(batchsize)\n",
    "    for steps, (x, y) in enumerate(dataset):\n",
    "      loss_history.append(self.trainStep(x,y))\n",
    "    return loss_history\n",
    "\n",
    "  def forward(self, x_input):\n",
    "    return self.toymodel(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "class Agent():\n",
    "    def __init__(self,runname, action_space, num_of_threads = 1, rom_name = 'CartPole-v1'):\n",
    "        self.rom_name = rom_name\n",
    "        self.eval_env = gym.make(rom_name)\n",
    "        self.action_space = action_space\n",
    "        self.num_of_threads = int(num_of_threads)\n",
    "        # Every number of steps equal to the epoch length, evalulation a greedy run of the Q function.\n",
    "        self.eval_epsilon = 0.05\n",
    "        self.eval_reps = 10\n",
    "        self.epoch_len = 50000\n",
    "        self.lives = 5\n",
    "\n",
    "        self.steps_taken = 0 \n",
    "        self.runname = runname\n",
    "        self.len_of_episode = 10000\n",
    "\n",
    "        #Set hyperparameters.\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_max = 1.0\n",
    "        self.epsilon_min = 0\n",
    "        self.epsilon_lag = 50000\n",
    "        self.annealing_time = 1000000\n",
    "        self.gamma = 0.99\n",
    "        self.max_memory_len = 1000000\n",
    "        self.batch_size = 32\n",
    "        self.steps_per_update = 4\n",
    "        self.reward_scaler = 0.01\n",
    "        self.target_update = 10000\n",
    "        self.window = 25\n",
    "        self.frameskip = 8 #Deprecated for the moment and needs updating\n",
    "        \n",
    "        #Initialize containers which will be prepared in thread_prep()\n",
    "        self.loss_history = []\n",
    "        self.action_history = []\n",
    "        self.state_history= []\n",
    "        self.next_state_history = []\n",
    "        self.reward_history = []\n",
    "        self.done_history = []\n",
    "        self.episodic_return = []\n",
    "        self.return_history = [] \n",
    "        self.env_container = []\n",
    "        self.threads = []\n",
    "        self.epsilon_schedule = []\n",
    "        self.grads = []\n",
    "        self.evaluations = []\n",
    "        self.thread_prep()\n",
    "        \n",
    "        #Initialize target and behavior network.\n",
    "        self.seed = 42\n",
    "        self.behavior = CONV_ANN(self.action_space, 84) #RAM_ANN(self.action_space, 128, self.frameskip, self.seed)\n",
    "        self.target = CONV_ANN(self.action_space, 84) #RAM_ANN(self.action_space, 128, self.frameskip,self.seed)\n",
    "        self.A3C = A3C(self.action_space, 128, self.frameskip, self.seed)\n",
    "\n",
    "    def clip_reward(self, reward):\n",
    "        if reward > 0:\n",
    "            return 1\n",
    "        elif reward == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def popback(self, state_block, incoming_state):\n",
    "        state_block.pop(0)\n",
    "        state_block.append(incoming_state)\n",
    "        return state_block\n",
    "\n",
    "    def gradient_update(self, \n",
    "                        runname,\n",
    "                        state_history, \n",
    "                        next_state_history,\n",
    "                        rewards_history,\n",
    "                        action_history,\n",
    "                        loss_history,\n",
    "                        model,\n",
    "                        target_model,\n",
    "                        gamma,\n",
    "                        batch_size,\n",
    "                        done_history,\n",
    "                        action_space):\n",
    "    \n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([state_history[i] for i in indices])\n",
    "            next_state_sample = np.array([next_state_history[i] for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
    "            future_rewards = target_model.toymodel.predict(next_state_sample)\n",
    "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values *(1-done_sample) - done_sample\n",
    "            masks = tf.one_hot(action_sample, len(action_space))\n",
    "            with tf.GradientTape() as tape:  \n",
    "                q_values = model.toymodel(state_sample)\n",
    "                q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = model.loss_fn(updated_q_values, q_actions)\n",
    "            loss_history = loss_history.append(loss)\n",
    "            grads = tape.gradient(loss, model.toymodel.trainable_variables)\n",
    "            model.toymodel.optimizer.apply_gradients(zip(grads, model.toymodel.trainable_variables))\n",
    "                      \n",
    "    def save_history(self,):   \n",
    "        runname = self.runname\n",
    "        np.save(runname + 'action_history',self.action_history)\n",
    "        np.save(runname + 'state_history', self.state_history)\n",
    "        np.save(runname + 'next_state_history', self.next_state_history)\n",
    "        np.save(runname + 'reward_history', self.reward_history)\n",
    "        np.save(runname + 'done_history', self.done_history)\n",
    "        np.save(runname + 'return_history', self.episodic_return)\n",
    "        np.save(runname + 'evaluations', self.evaluations)\n",
    "        np.save(runname + 'loss_history', self.loss_history)\n",
    "        self.behavior.toymodel.save(runname+'_behavior')\n",
    "        self.target.toymodel.save(runname+'_target')\n",
    "\n",
    "    def RGB_preprocess(self, action, env, frameskips):\n",
    "        state_output = []\n",
    "        reward = 0\n",
    "        for i in range(frameskips):\n",
    "            s, r, d, info = env.step(action)\n",
    "            reward += r\n",
    "            s = cv2.cvtColor(s, cv2.COLOR_RGB2GRAY)\n",
    "            s = cv2.resize(s, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "            state_output.append(s/255.0)\n",
    "        #return np.max(np.dstack(state_output), axis=2), reward, d, info #For max stacking\n",
    "        return np.dstack(state_output).astype('float16'), reward, d, info #For frame stacking\n",
    "\n",
    "    def preprocess(self, state):\n",
    "        #return [state[0]/4.8, state[1], state[2]/0.418, state[3]]\n",
    "        return state/255.0\n",
    "\n",
    "    def memory_manager(self,array, mem_size):\n",
    "        num_delete = len(array) - mem_size\n",
    "        if num_delete < 0:\n",
    "            None\n",
    "        else:\n",
    "            del array[:num_delete]\n",
    "            \n",
    "    def piecewise_epsilon(self, steps_taken, lag, annealingtime, ep_min, ep_max): #returns epsilon\n",
    "        anneal_slope= (ep_min-ep_max)/(lag+annealingtime-lag)\n",
    "        if steps_taken < lag: return ep_max\n",
    "        if (steps_taken >= lag) and (steps_taken < (lag+annealingtime)): return anneal_slope*steps_taken+(ep_max-anneal_slope*lag)\n",
    "        else: return ep_min\n",
    "\n",
    "    def sliding_average(self, array, n):\n",
    "        output = []\n",
    "        for i in range(len(array)):\n",
    "            try:\n",
    "                output.append(np.average(array[i:i+n]))\n",
    "            except IndexError:\n",
    "                break\n",
    "        return output\n",
    "    \n",
    "    def thread_prep(self):\n",
    "        self.action_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.state_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.next_state_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.reward_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.done_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.episodic_return = [[] for i in range(self.num_of_threads)]\n",
    "        self.return_history = [[] for i in range(self.num_of_threads)]\n",
    "        self.env_container = [gym.make(self.rom_name) for i in range(self.num_of_threads)]\n",
    "        self.steps_taken = [0 for i in range(self.num_of_threads)]\n",
    "        self.evaluations = [0 for i in range(self.num_of_threads)]\n",
    "        self.epsilon_schedule = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "        \n",
    "        \n",
    "    def agent_thread(self, thread_num, num_training_steps, seed=42):\n",
    "        logging.info(\"Thread %s: starting\", thread_num)\n",
    "        env = self.env_container[thread_num]\n",
    "        #self.env_container[thread_num].seed(seed)\n",
    "        self.RGB_episode(num_training_steps, env, thread_num, epsilon)\n",
    "        self.episode(num_training_steps*self.steps_per_update, self.env_container[thread_num], thread_num, self.epsilon_schedule[thread_num])\n",
    "        logging.info(\"Thread %s: finishing\", thread_num)\n",
    "     \n",
    "    def start_threads(self, num_training_steps=1):\n",
    "        format = \"%(asctime)s: %(message)s\"\n",
    "        logging.basicConfig(format=format, level=logging.INFO, datefmt=\"%H:%M:%S\")\n",
    "        grads = []\n",
    "        for i in range(self.num_of_threads):\n",
    "            logging.info(\"Main    : create and start thread %d.\", i)\n",
    "            x = threading.Thread(target=self.agent_thread, args=(i, num_training_steps))\n",
    "            self.threads.append(x)\n",
    "            x.start()\n",
    "                \n",
    "        while len(self.state_history[0]) < num_training_steps:\n",
    "            self.plot_data()\n",
    "            time.sleep(10)\n",
    "                            \n",
    "        for j in self.threads:\n",
    "            j.join()\n",
    "\n",
    "        self.plot_data()\n",
    "        \n",
    "    def plot_data(self,):\n",
    "        clear_output()\n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.episodic_return[i], label='Thread ' + str(i))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Return')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            plt.plot(self.sliding_average(self.episodic_return[i], self.window), label='Thread '+str(i))\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel(str(self.window)+'-averaged Return')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.plot(self.evaluations)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Evaluation Return')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(10,2))\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.yscale('log')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10,2))\n",
    "        for i in range(self.num_of_threads):\n",
    "            Y = [self.piecewise_epsilon(j, self.epsilon_lag, self.annealing_time, self.epsilon_schedule[i], self.epsilon_max) for j in np.arange(self.steps_taken[i])]\n",
    "            plt.plot(Y, label='Thread '+str(i))\n",
    "        plt.xlabel('Steps taken')\n",
    "        plt.ylabel('Epsilon')\n",
    "        plt.legend(loc=7)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def evaluate_NN(self, env, epsilon, replicates):\n",
    "        av_returns = []\n",
    "        for i in range(replicates):\n",
    "            s = env.reset()\n",
    "            s, episode_return, done, info = self.RGB_preprocess(1, env, self.frameskip)\n",
    "            episode_return = 0\n",
    "            done = False\n",
    "            eval_life = self.lives\n",
    "            while True:\n",
    "                #Choose epsilon greedy action.\n",
    "                if np.random.random() < epsilon:\n",
    "                    a = np.random.choice(np.arange(len(self.action_space)))\n",
    "                else: \n",
    "                    a_probs = self.behavior.toymodel(np.expand_dims(s,0), training=False)\n",
    "                    a = tf.argmax(a_probs[0]).numpy()\n",
    "                #Collect information on next state.\n",
    "                s, reward, done, info = self.RGB_preprocess(self.action_space[a], env, self.frameskip)\n",
    "                episode_return += reward\n",
    "                #If a life was lost, don't end the episode. Use fire button to go to next life.\n",
    "                if not (int(info['ale.lives']) == eval_life):                                         \n",
    "                    eval_life = int(info['ale.lives'])\n",
    "                    s, reward, done, info = self.RGB_preprocess(1, env, self.frameskip)\n",
    "                    av_returns.append(episode_return)  \n",
    "                    episode_return = 0 \n",
    "                if done:\n",
    "                    av_returns.append(episode_return)   \n",
    "                    break   \n",
    "        self.evaluations.append(np.average(av_returns))\n",
    "        \n",
    "    def RGB_episode(self, num_training_steps, thread_num, epsilon):    #Double Deep Q\n",
    "        #np.random.seed(self.seed)\n",
    "        epsilon = self.piecewise_epsilon(self.steps_taken[thread_num], self.epsilon_lag, self.annealing_time, self.epsilon_min, self.epsilon_max)\n",
    "        while self.steps_taken[thread_num] < num_training_steps:\n",
    "            #self.env_container[thread_num]env.seed(self.seed)\n",
    "            lives = self.lives     #5 for Breakout, 3 for Space Invaders\n",
    "            if len(self.episodic_return[thread_num])%25==0: self.plot_data()\n",
    "            epi_return = 0 \n",
    "            self.env_container[thread_num].reset()\n",
    "            s, reward, done, info = self.RGB_preprocess(1, self.env_container[thread_num], self.frameskip)\n",
    "            #Enter the loop.\n",
    "            while self.steps_taken[thread_num] < num_training_steps:\n",
    "                if self.steps_taken[thread_num]%self.epoch_len==0 and self.steps_taken[thread_num]>1: \n",
    "                    self.evaluate_NN(self.eval_env, self.eval_epsilon, self.eval_reps)\n",
    "                    self.save_history()\n",
    "                    \n",
    "                #Choose an action from according to epsilson-greedy policy.  \n",
    "                if np.random.random() < epsilon:\n",
    "                    a = np.random.choice(np.arange(len(self.action_space)))\n",
    "                else: \n",
    "                    a_probs = self.behavior.toymodel(np.expand_dims(s,0), training=False)\n",
    "                    a = tf.argmax(a_probs[0]).numpy()\n",
    "                    \n",
    "                s_prime, reward, done, info = self.RGB_preprocess(self.action_space[a], self.env_container[thread_num], self.frameskip)\n",
    "                epi_return += reward\n",
    "              \n",
    "\n",
    "                    \n",
    "                #Save to history\n",
    "                self.reward_history[thread_num].append(self.clip_reward(reward)*self.reward_scaler)\n",
    "                self.state_history[thread_num].append(s)\n",
    "                self.action_history[thread_num].append(a)\n",
    "                self.next_state_history[thread_num].append(s_prime)\n",
    "                self.done_history[thread_num].append(done)\n",
    "\n",
    "                                #Restart when the end of the episode is reached.  \n",
    "                if done:                                                                              #FUNCTIONIZE!\n",
    "                    #Set the last frame to -1 to discourage dying.                                             \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    #self.reward_history[thread_num][-1] = -1\n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    break\n",
    "                    \n",
    "                if not (int(info['ale.lives']) == lives):                                         \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    epi_return = 0\n",
    "                    s, reward, done, info = self.RGB_preprocess(1, self.env_container[thread_num], self.frameskip)\n",
    "                    lives = int(info['ale.lives'])\n",
    "                    \n",
    "                if self.steps_taken[thread_num]>self.batch_size and self.steps_taken[thread_num]%self.steps_per_update==0:\n",
    "                      self.gradient_update(self.runname,\n",
    "                                        self.state_history[thread_num], \n",
    "                                        self.next_state_history[thread_num],\n",
    "                                        self.reward_history[thread_num],\n",
    "                                        self.action_history[thread_num],\n",
    "                                        self.loss_history,\n",
    "                                        self.behavior,\n",
    "                                        self.target,\n",
    "                                        self.gamma,\n",
    "                                        self.batch_size,\n",
    "                                        self.done_history[thread_num],\n",
    "                                        self.action_space) \n",
    "\n",
    "\n",
    "                if self.steps_taken[thread_num]%self.target_update==0:\n",
    "                    self.target.toymodel.set_weights(self.behavior.toymodel.get_weights()) \n",
    "\n",
    "                s = s_prime\n",
    "\n",
    "                self.steps_taken[thread_num] += 1\n",
    "                \n",
    "                self.memory_manager(self.action_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.next_state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.reward_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.done_history[thread_num], self.max_memory_len)\n",
    "                                  \n",
    "    def A3C_episode(self, num_training_steps, thread_num):    #Double Deep Q\n",
    "        #np.random.seed(self.seed)\n",
    "        while self.steps_taken[thread_num] < num_training_steps:\n",
    "            env_container[thread_num].seed(self.seed)\n",
    "            epi_return = 0 \n",
    "            steps = 0\n",
    "            lives = 5     #5 for Breakout, 3 for Space Invaders\n",
    "            s = self.preprocess(env_container[thread_num].reset())\n",
    "            s, reward, done, info = env_container[thread_num].step(1)\n",
    "            done = False\n",
    "            #Enter the loop.\n",
    "            if len(self.episodic_return[thread_num])%25==0: self.plot_data()\n",
    "            while self.steps_taken[thread_num] < num_training_steps:\n",
    "\n",
    "                #Break the loop if the maximum number of training examples have been reached.\n",
    "                if self.steps_taken[thread_num] >= num_training_steps:\n",
    "                    break\n",
    "                #Choose an action from according to epsilson-greedy policy.  \n",
    "                a_probs, value = self.A3C.toymodel(np.expand_dims(s,0), training=False)\n",
    "                a = np.random.choice(self.action_space,p=tf.nn.softmax(a_probs[0]).numpy())\n",
    "                \n",
    "                s_prime, reward, done, info = self.RGB_preprocess(self.action_space[a], self.env_container[thread_num], self.frameskip)\n",
    "                s_prime = self.preprocess(s_prime)\n",
    "                epi_return += reward\n",
    "\n",
    "                #Save to history\n",
    "                self.reward_history[thread_num].append(reward*self.reward_scaler)\n",
    "                self.state_history[thread_num].append(s)\n",
    "                self.action_history[thread_num].append(a)\n",
    "                self.next_state_history[thread_num].append(s_prime)\n",
    "                self.done_history[thread_num].append(done)\n",
    "                \n",
    "                #Restart when the end of the episode is reached.  \n",
    "                if done:                                                                              #FUNCTIONIZE!\n",
    "                    #Set the last frame to -1 to discourage dying.                                             \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    #self.reward_history[thread_num][-1] = -1\n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    break\n",
    "                    \n",
    "                if not (int(info['ale.lives']) == lives):                                         \n",
    "                    self.done_history[thread_num][-1] = True \n",
    "                    self.episodic_return[thread_num].append(epi_return)\n",
    "                    epi_return = 0\n",
    "                    s, reward, done, info = self.RGB_preprocess(1, self.env_container[thread_num], self.frameskip)\n",
    "                    lives = int(info['ale.lives'])          \n",
    "                                                \n",
    "                if self.steps_taken[thread_num]>self.batch_size and self.steps_taken[thread_num]%self.steps_per_update==0:\n",
    "                    #Enable option for experience replay by random sampling or total memory sampling.\n",
    "                    # Get indices of samples for replay buffers\n",
    "                    indices = np.random.choice(range(len(self.done_history[thread_num])), size=self.batch_size)\n",
    "                    # Using list comprehension to sample from replay buffer\n",
    "                    state_sample = np.array([self.state_history[thread_num][i] for i in indices])\n",
    "                    next_state_sample = np.array([self.next_state_history[thread_num][i] for i in indices])\n",
    "                    rewards_sample = [self.reward_history[thread_num][i] for i in indices]\n",
    "                    action_sample = [self.action_history[thread_num][i] for i in indices]\n",
    "                    done_sample = tf.convert_to_tensor([float(self.done_history[thread_num][i]) for i in indices])\n",
    "                                        \n",
    "                    with tf.GradientTape() as tape:\n",
    "                        future_rewards = self.A3C.toymodel(next_state_sample)[1][:,0]\n",
    "                        future_rewards = future_rewards *(1-done_sample) - done_sample                    \n",
    "                        discounted_reward = rewards_sample + self.gamma * future_rewards   \n",
    "                        logits, values = self.A3C.toymodel(state_sample)   \n",
    "                        values = values[:,0]\n",
    "                        advantage = discounted_reward-values\n",
    "                        masks = tf.one_hot(action_sample, len(self.action_space))\n",
    "                        policy = tf.nn.softmax(logits)\n",
    "                        entropy = tf.reduce_sum(policy * tf.math.log(policy+1e-20),axis=1)\n",
    "                        value_loss = advantage**2\n",
    "                        policy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=masks, logits=logits)\n",
    "                        policy_loss *= tf.stop_gradient(advantage)\n",
    "                        policy_loss -= 0.0005 * entropy\n",
    "                        total_loss = (0.5 * value_loss + policy_loss)\n",
    "                    loss_history = self.loss_history[thread_num].append(np.mean(total_loss))\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(total_loss, self.A3C.toymodel.trainable_weights)\n",
    "                    # Push local gradients to global model\n",
    "                    self.A3C.toymodel.optimizer.apply_gradients(zip(grads, self.A3C.toymodel.trainable_weights))\n",
    "                    steps += 1                 \n",
    "                s = s_prime\n",
    "\n",
    "                self.steps_taken[thread_num] += 1\n",
    "                self.memory_manager(self.action_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.next_state_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.reward_history[thread_num], self.max_memory_len)\n",
    "                self.memory_manager(self.done_history[thread_num], self.max_memory_len)          \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atari_action_space = [0,2,3]\n",
    "SMB_action_space = [[0,0,0,0,0,0,1,0,0],\n",
    "                [0,0,0,0,0,0,0,1,0],\n",
    "                [0,0,0,0,0,0,1,0,1],\n",
    "                [0,0,0,0,0,0,0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define environments\n",
    "#env = retro.make(game='SuperMarioBros-Nes')\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent('210522_An_DQN', atari_action_space, num_of_threads = 1, rom_name=\"BreakoutNoFrameskip-v4\")\n",
    "#agent = Agent('210422_1_SMB_DQN', SMB_action_space, num_of_threads = 1, rom_name='SuperMarioBros-Nes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 84, 84, 4)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 1,685,667\n",
      "Trainable params: 1,685,667\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.epsilon = 1.0\n",
    "agent.epsilon_max = 1.0\n",
    "agent.epsilon_min = 0.1\n",
    "agent.epsilon_lag = 10000\n",
    "agent.annealing_time = 1000000\n",
    "agent.len_of_episode = 10000\n",
    "agent.gamma = 0.99\n",
    "agent.max_memory_len = 100000\n",
    "agent.batch_size = 32\n",
    "agent.steps_per_update = 4\n",
    "agent.reward_scaler = 1\n",
    "agent.target_update = 50000\n",
    "#agent.epsilon_schedule = [0.05, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "agent.frameskip = 4\n",
    "agent.lives = 2 #Breakout is 5, SMB = 2\n",
    "#Evaluation parms\n",
    "agent.eval_epsilon = 0.05\n",
    "agent.eval_reps = 10\n",
    "agent.epoch_len = 5000\n",
    "#agent.episode(10000000, env1)\n",
    "#LR is 0.0000625\n",
    "agent.behavior.toymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0fec5e5b4897>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_memory_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRGB_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-4e37827e5988>\u001b[0m in \u001b[0;36mRGB_episode\u001b[1;34m(self, num_training_steps, thread_num, epsilon)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_taken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_taken\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps_per_update\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m                       self.gradient_update(self.runname,\n\u001b[0m\u001b[0;32m    316\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_state_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthread_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-4e37827e5988>\u001b[0m in \u001b[0;36mgradient_update\u001b[1;34m(self, runname, state_history, next_state_history, rewards_history, action_history, loss_history, model, target_model, gamma, batch_size, done_history, action_space)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0maction_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0maction_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mdone_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_history\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mfuture_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoymodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mupdated_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards_sample\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mupdated_q_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdated_q_values\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mdone_sample\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdone_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1567\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1569\u001b[1;33m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[0;32m   1570\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1571\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1105\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \"\"\"\n\u001b[0;32m   1694\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4039\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4040\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4041\u001b[1;33m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[0;32m   4042\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4043\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3369\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3370\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3373\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m       \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mto\u001b[0m \u001b[0mspecialize\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m     \"\"\"\n\u001b[1;32m-> 2938\u001b[1;33m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[0;32m   2939\u001b[0m         *args, **kwargs)\n\u001b[0;32m   2940\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2904\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3362\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3363\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3364\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3365\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3297\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3299\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3300\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3301\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    317\u001b[0m       \u001b[1;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m       \u001b[1;31m# forwarding.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m       \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1810\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1812\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m_range\u001b[1;34m(start, limit, delta, name)\u001b[0m\n\u001b[0;32m   7310\u001b[0m       \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7311\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7312\u001b[1;33m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[0;32m   7313\u001b[0m         \"Range\", start=start, limit=limit, delta=delta, name=name)\n\u001b[0;32m   7314\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    740\u001b[0m       \u001b[1;31m# Add Op to graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m       \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[0minp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         compute_device)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3485\u001b[0m           op_def=op_def)\n\u001b[1;32m-> 3486\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[1;34m(self, op, compute_device)\u001b[0m\n\u001b[0;32m   3539\u001b[0m     \u001b[1;31m# Apply a kernel label if one has been specified for this op type.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3540\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3541\u001b[1;33m       \u001b[0mkernel_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_op_to_kernel_label_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3542\u001b[0m       op._set_attr(\"_kernel\",  # pylint: disable=protected-access\n\u001b[0;32m   3543\u001b[0m                    attr_value_pb2.AttrValue(s=compat.as_bytes(kernel_label)))\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\VGBOT\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2377\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2378\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2379\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.max_memory_len = 100000\n",
    "agent.RGB_episode(10000, 0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
